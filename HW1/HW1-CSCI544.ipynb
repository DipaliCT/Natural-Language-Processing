{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dipal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "\n",
    "#!pip install bs4\n",
    "#!pip install contractions \n",
    "#!pip install scikit-learn\n",
    "\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#remove warnings in output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipping rows in dataset which give error \n",
    "#reference: https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data\n",
    "df = pd.read_table(\"amazon_reviews_us_Beauty_v1_00.tsv\", on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"star_rating\", \"review_body\"]]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column named \"class\" to define class 1,2 and 3\n",
    "#reference: https://sparkbyexamples.com/pandas/pandas-apply-with-lambda-examples/#:~:text=Apply%20Lambda%20Expression%20to%20Single,x%3Ax%2D2)%20\n",
    "df[\"class\"] = df[\"star_rating\"].apply(lambda x : 3 if str(x) > '3' else 2 if str(x) == '3' else 1)\n",
    "\n",
    "#select 20000 reviews from each class\n",
    "#reference: https://stackoverflow.com/questions/67174746/sklearn-take-only-few-records-from-each-target-class\n",
    "df = df.groupby('class').sample(n=20000, replace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review character length before and after cleaning:  273.44127010584214 , 263.40793333333335\n"
     ]
    }
   ],
   "source": [
    "df_train = df[[\"review_body\", \"class\"]]\n",
    "\n",
    "charlenpre = df_train['review_body'].str.len().mean()\n",
    "\n",
    "#review to lower case\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x : str(x).lower())\n",
    "\n",
    "#remove html tags\n",
    "#reference: https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: BeautifulSoup(str(x)).get_text())\n",
    "\n",
    "#remove url\n",
    "#reference: https://stackoverflow.com/questions/51994254/removing-url-from-a-column-in-pandas-dataframe\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "#remove non-alphabetical words\n",
    "df_train['review_body'] = df_train['review_body'].replace('[^a-zA-Z ]', '', regex=True)\n",
    "\n",
    "#remove extra spaces\n",
    "df_train['review_body'] = df_train['review_body'].str.strip()\n",
    "\n",
    "#perform contractions\n",
    "#reference: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: contractions.fix(str(x)))\n",
    "\n",
    "charlenpost = df_train['review_body'].str.len().mean()\n",
    "\n",
    "print(\"Average review character length before and after cleaning: \", charlenpre, \",\", charlenpost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dipal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "charlenpre = df_train['review_body'].str.len().mean()\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "#remove the words which are present in stopwords\n",
    "#reference: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: ' '.join([word for word in x.split() \n",
    "                                                                            if word not in (stopwords)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review character length before and after pre-processing:  263.40793333333335 ,  152.23136666666667\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#perform lemmatization of words\n",
    "#reference: 1. https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "#           2. https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#verbs\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, pos=\"v\") \n",
    "                                                                            for word in x.split()]))\n",
    "\n",
    "#noun\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, pos=\"n\") \n",
    "                                                                            for word in x.split()]))\n",
    "\n",
    "#adjectives\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, pos=\"a\") \n",
    "                                                                            for word in x.split()]))\n",
    "\n",
    "#adverbs\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, pos=\"r\") \n",
    "                                                                            for word in x.split()]))\n",
    "\n",
    "#satellite adjectives\n",
    "df_train['review_body'] = df_train['review_body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, pos=\"s\") \n",
    "                                                                            for word in x.split()]))\n",
    "\n",
    "charlenpost = df_train['review_body'].str.len().mean()\n",
    "\n",
    "print(\"Average review character length before and after pre-processing: \",charlenpre, \", \", charlenpost)\n",
    "\n",
    "#df_train.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "#tf-idf feature extraction of input\n",
    "#reference: https://stackoverflow.com/questions/37593293/how-to-get-tfidf-with-pandas-dataframe\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(df_train['review_body'])\n",
    "\n",
    "#vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron model output:\n",
      "class1:  0.6009032564772997 ,  0.632 ,  0.6160594614353601\n",
      "class2:  0.5062782521346058 ,  0.504 ,  0.5051365572538211\n",
      "class3:  0.6688533193387562 ,  0.63725 ,  0.6526693125080015\n",
      "average: 0.5920116093168872 ,  0.5910833333333333 ,  0.5912884437323942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#split data into training and test\n",
    "#reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(vector, df_train['class'], stratify=df_train['class'], \n",
    "                                                test_size=0.2, random_state=42)\n",
    "\n",
    "#Perceptron model training\n",
    "#reference: https://python-course.eu/machine-learning/perceptron-class-in-sklearn.php\n",
    "model_p = Perceptron(random_state=42)\n",
    "model_p.fit(Xtrain, Ytrain)\n",
    "\n",
    "#Testing the model\n",
    "Ypred = model_p.predict(Xtest)\n",
    "\n",
    "precision_score_p = precision_score(Ytest, Ypred, average=None)\n",
    "recall_score_p = recall_score(Ytest, Ypred, average=None)\n",
    "f1_score_p = f1_score(Ytest, Ypred, average=None)\n",
    "\n",
    "print(\"Perceptron model output:\")\n",
    "print(\"class1: \", precision_score_p[0], \", \", recall_score_p[0], \", \", f1_score_p[0])\n",
    "print(\"class2: \", precision_score_p[1], \", \", recall_score_p[1], \", \", f1_score_p[1])\n",
    "print(\"class3: \", precision_score_p[2], \", \", recall_score_p[2], \", \", f1_score_p[2])\n",
    "print(\"average:\", precision_score(Ytest, Ypred, average='weighted'),\", \", recall_score(Ytest, Ypred, average='weighted'),\n",
    "     \", \", f1_score(Ytest, Ypred, average='weighted'))\n",
    "\n",
    "\n",
    "#print(\"training dataresults: \")\n",
    "#print(classification_report(model_p.predict(Xtrain), Ytrain))\n",
    "\n",
    "#print(\"testing dataresults: \")\n",
    "#print(classification_report(Ypred, Ytest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model output:\n",
      "class1:  0.6601401982112642 ,  0.68275 ,  0.67125476219737\n",
      "class2:  0.5764705882352941 ,  0.539 ,  0.5571059431524549\n",
      "class3:  0.7215619694397284 ,  0.74375 ,  0.7324879970454267\n",
      "average: 0.6527242519620956 ,  0.6551666666666667 ,  0.6536162341317505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Linear SVM model training\n",
    "#reference: 1. https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/\n",
    "#           2. https://stackoverflow.com/questions/27912872/what-is-the-difference-between-svc-and-svm-in-scikit-learn\n",
    "model_s = LinearSVC(random_state=42) \n",
    "model_s.fit(Xtrain, Ytrain)\n",
    "\n",
    "#Testing the model\n",
    "Ypred = model_s.predict(Xtest)\n",
    "\n",
    "precision_score_s = precision_score(Ytest, Ypred, average=None)\n",
    "recall_score_s = recall_score(Ytest, Ypred, average=None)\n",
    "f1_score_s = f1_score(Ytest, Ypred, average=None)\n",
    "\n",
    "print(\"SVM model output:\")\n",
    "print(\"class1: \", precision_score_s[0], \", \", recall_score_s[0], \", \", f1_score_s[0])\n",
    "print(\"class2: \", precision_score_s[1], \", \", recall_score_s[1], \", \", f1_score_s[1])\n",
    "print(\"class3: \", precision_score_s[2], \", \", recall_score_s[2], \", \", f1_score_s[2])\n",
    "print(\"average:\", precision_score(Ytest, Ypred, average='weighted'), \", \", recall_score(Ytest, Ypred, average='weighted'), \n",
    "      \", \", f1_score(Ytest, Ypred, average='weighted'))\n",
    "\n",
    "\n",
    "#print(\"training dataresults: \")\n",
    "#print(classification_report(model_s.predict(Xtrain), Ytrain))\n",
    "\n",
    "#print(\"testing dataresults: \")\n",
    "#print(classification_report(Ypred, Ytest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model output:\n",
      "class1:  0.6754726126999515 ,  0.69675 ,  0.6859463450652227\n",
      "class2:  0.5952929137886928 ,  0.58175 ,  0.5884435453281072\n",
      "class3:  0.7465321563682219 ,  0.74 ,  0.7432517263025737\n",
      "average: 0.6724325609522888 ,  0.6728333333333333 ,  0.6725472055653011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Logistic Regression model training\n",
    "#reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "model_l = LogisticRegression(random_state=42) \n",
    "model_l.fit(Xtrain, Ytrain)\n",
    "\n",
    "#Testing the model\n",
    "Ypred = model_l.predict(Xtest)\n",
    "\n",
    "precision_score_l = precision_score(Ytest, Ypred, average=None)\n",
    "recall_score_l = recall_score(Ytest, Ypred, average=None)\n",
    "f1_score_l = f1_score(Ytest, Ypred, average=None)\n",
    "\n",
    "print(\"Logistic Regression model output:\")\n",
    "print(\"class1: \", precision_score_l[0], \", \", recall_score_l[0], \", \", f1_score_l[0])\n",
    "print(\"class2: \", precision_score_l[1], \", \", recall_score_l[1], \", \", f1_score_l[1])\n",
    "print(\"class3: \", precision_score_l[2], \", \", recall_score_l[2], \", \", f1_score_l[2])\n",
    "print(\"average:\", precision_score(Ytest, Ypred, average='weighted'),\", \", recall_score(Ytest, Ypred, average='weighted'), \n",
    "      \", \", f1_score(Ytest, Ypred, average='weighted'))\n",
    "\n",
    "\n",
    "#print(\"training dataresults: \")\n",
    "#print(classification_report(model_l.predict(Xtrain), Ytrain))\n",
    "\n",
    "#print(\"testing dataresults: \")\n",
    "#print(classification_report(Ypred, Ytest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes model output:\n",
      "class1:  0.6812717071867486 ,  0.6375 ,  0.6586594343277798\n",
      "class2:  0.5502413339183853 ,  0.627 ,  0.5861182519280206\n",
      "class3:  0.7512841308461746 ,  0.69475 ,  0.7219119366151447\n",
      "average: 0.6609323906504362 ,  0.6530833333333333 ,  0.6555632076236484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_n = MultinomialNB() \n",
    "model_n.fit(Xtrain, Ytrain)\n",
    "\n",
    "#Testing the model\n",
    "Ypred = model_n.predict(Xtest)\n",
    "\n",
    "precision_score_n = precision_score(Ytest, Ypred, average=None)\n",
    "recall_score_n = recall_score(Ytest, Ypred, average=None)\n",
    "f1_score_n = f1_score(Ytest, Ypred, average=None)\n",
    "\n",
    "print(\"Multinomial Naive Bayes model output:\")\n",
    "print(\"class1: \", precision_score_n[0], \", \", recall_score_n[0], \", \", f1_score_n[0])\n",
    "print(\"class2: \", precision_score_n[1], \", \", recall_score_n[1], \", \", f1_score_n[1])\n",
    "print(\"class3: \", precision_score_n[2], \", \", recall_score_n[2], \", \", f1_score_n[2])\n",
    "print(\"average:\", precision_score(Ytest, Ypred, average='weighted'),\", \", recall_score(Ytest, Ypred, average='weighted'), \n",
    "      \", \", f1_score(Ytest, Ypred, average='weighted'))\n",
    "\n",
    "#print(\"training dataresults: \")\n",
    "#print(classification_report(model_n.predict(Xtrain), Ytrain))\n",
    "\n",
    "#print(\"testing dataresults: \")\n",
    "#print(classification_report(Ypred, Ytest))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
