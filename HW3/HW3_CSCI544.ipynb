{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23e4c35",
   "metadata": {},
   "source": [
    "Use Jupyter notebook\n",
    "Gensim version = 4.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ef9000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dipal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip install gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score  \n",
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#remove warnings in output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9939a",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3285538",
   "metadata": {},
   "source": [
    "Getting dataset from input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cf5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"amazon_reviews_us_Beauty_v1_00.tsv\", on_bad_lines='skip')\n",
    "df = df[[\"star_rating\", \"review_body\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca33c61",
   "metadata": {},
   "source": [
    "Created new column class to separate classes as Class 1, 2 and 3 in according to the star rating. Removed the grabage data.\n",
    "Sampled 20000 instances of each class to get dataset of 60000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3c5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"star_rating\"].apply(lambda x : 3 if str(x) == '4' or str(x) == '5' \n",
    "                                            else 2 if str(x) == '3' \n",
    "                                            else 1 if str(x) == '2' or str(x) == '1'\n",
    "                                            else 0)\n",
    "\n",
    "df.drop(df[(df['class'] == 0)].index, inplace=True)\n",
    "df = df.groupby('class').sample(n=20000, replace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f1387",
   "metadata": {},
   "source": [
    "<b>Preprocessing of data</b> -\n",
    "Removed html tags, urls, non-alphabetical letters and extra spaces from the reviews. Performed contractions on every word in review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7bde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html tags\n",
    "df['review_body'] = df['review_body'].apply(lambda x: BeautifulSoup(str(x)).get_text())\n",
    "\n",
    "#remove url\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "#remove non-alphabetical words\n",
    "df['review_body'] = df['review_body'].replace('[^a-zA-Z ]', '', regex=True)\n",
    "\n",
    "#remove extra spaces\n",
    "df['review_body'] = df['review_body'].str.strip()\n",
    "\n",
    "#perform contractions\n",
    "df['review_body'] = df['review_body'].apply(lambda x: contractions.fix(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71567bc",
   "metadata": {},
   "source": [
    "Created new column to save reviews as a review split into list of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa1bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body1'] = df['review_body'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf9f9d",
   "metadata": {},
   "source": [
    "Removed the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559e6692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dipal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "df['review_body'] = df['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758215ce",
   "metadata": {},
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e0af8",
   "metadata": {},
   "source": [
    "### Q.2 a) Pretrained Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f7047",
   "metadata": {},
   "source": [
    "Used existing api.load() method to load pre-trained dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b428e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ca4ff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Queen', 0.4929387867450714),\n",
       " ('Tupou_V.', 0.45174285769462585),\n",
       " ('Oprah_BFF_Gayle', 0.4422132968902588),\n",
       " ('Jackson', 0.440250426530838),\n",
       " ('NECN_Alison', 0.4331282675266266)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wv.most_similar(positive=['King', 'Woman'], negative=['Man'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "590d1af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'excellent' and 'outstanding' :0.55674857\n",
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('oustanding', 0.750198483467102),\n",
       " ('exceptional', 0.7280517220497131),\n",
       " ('terrific', 0.7081279158592224),\n",
       " ('superb', 0.6691538095474243),\n",
       " ('exemplary', 0.6476037502288818)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wv.similarity('excellent', 'outstanding')\n",
    "print(\"Similarity between \\'excellent\\' and \\'outstanding\\' :\" + str(result))\n",
    "result = wv.most_similar(positive=['excellent', 'outstanding'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5e62ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'Amazing' and 'Good' :0.38268512\n",
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Awesome', 0.6103745102882385),\n",
       " ('Terrific', 0.5929716229438782),\n",
       " ('Awful', 0.5901114344596863),\n",
       " ('Wonderful', 0.5890116095542908),\n",
       " ('Bad', 0.582302987575531)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wv.similarity('Amazing', 'Good')\n",
    "print(\"Similarity between \\'Amazing\\' and \\'Good\\' :\" + str(result))\n",
    "result = wv.most_similar(positive=['Amazing', 'Good'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad2224c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'hair' and 'straightener' :0.45512608\n",
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tresses', 0.7045788168907166),\n",
       " ('straightening_iron', 0.6670017838478088),\n",
       " ('blowdry', 0.6644049286842346),\n",
       " ('straighteners', 0.6463422179222107),\n",
       " ('curly_hair', 0.6419693827629089)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wv.similarity('hair', 'straightener')\n",
    "print(\"Similarity between \\'hair\\' and \\'straightener\\' :\" + str(result))\n",
    "result = wv.most_similar(positive=['hair', 'straightener'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c9bdfab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nap', 0.5254397392272949),\n",
       " ('sleeping', 0.5071698427200317),\n",
       " ('naps', 0.5010553002357483),\n",
       " ('restful_sleep', 0.5006630420684814),\n",
       " ('doze', 0.4848100543022156)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = wv.most_similar(positive=['sleep', 'morning'], negative=['night'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86f8eb",
   "metadata": {},
   "source": [
    "### Q.2 b) Trained Word2Vec model from local dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a97c2",
   "metadata": {},
   "source": [
    "Built the vocab for model using the parameters given in question and reviews that are split into words. Trained the model to create a word vector of given vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c47f904b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7014953, 8649930)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "model = Word2Vec(vector_size=300, window=13, min_count=9, workers=num_workers)\n",
    "model.build_vocab(df.review_body1, progress_per=10000)\n",
    "model.train(df.review_body1, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8335efb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'Woman' does not exist in our vocabulary. Hence cannot find the similarities\n"
     ]
    }
   ],
   "source": [
    "# result = model.wv.most_similar(positive=['King', 'Woman'], negative=['Man'])\n",
    "# result\n",
    "print('Word \\'Woman\\' does not exist in our vocabulary. Hence cannot find the similarities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "32e8c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'excellent' and 'outstanding' :0.6133924\n",
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Quick', 0.8102356195449829),\n",
       " ('Wonderful', 0.7927247881889343),\n",
       " ('par', 0.779434323310852),\n",
       " ('speedy', 0.7787030339241028),\n",
       " ('competitive', 0.7593490481376648)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.wv.similarity('excellent', 'outstanding')\n",
    "print(\"Similarity between \\'excellent\\' and \\'outstanding\\' :\" + str(result))\n",
    "result = model.wv.most_similar(positive=['excellent', 'outstanding'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "245bce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'Amazing' and 'Good' :0.51051456\n",
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Excellent', 0.8870605826377869),\n",
       " ('Great', 0.8442416191101074),\n",
       " ('Wonderful', 0.8369162678718567),\n",
       " ('Awesome', 0.8105168342590332),\n",
       " ('Decent', 0.8068863153457642)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.wv.similarity('Amazing', 'Good')\n",
    "print(\"Similarity between \\'Amazing\\' and \\'Good\\' :\" + str(result))\n",
    "result = model.wv.most_similar(positive=['Amazing', 'Good'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de1eab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'hair' and 'straightener' :0.5208654\n",
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('straighten', 0.8750419616699219),\n",
       " ('wavy', 0.8607210516929626),\n",
       " ('straightening', 0.8529598116874695),\n",
       " ('curly', 0.8519555330276489),\n",
       " ('straight', 0.851128339767456)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.wv.similarity('hair', 'straightener')\n",
    "print(\"Similarity between \\'hair\\' and \\'straightener\\' :\" + str(result))\n",
    "result = model.wv.most_similar(positive=['hair', 'straightener'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9c182842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sleeping', 0.80987948179245),\n",
       " ('wake', 0.803053617477417),\n",
       " ('woke', 0.7903323173522949),\n",
       " ('bed', 0.7244474291801453),\n",
       " ('arms', 0.7103226780891418)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.wv.most_similar(positive=['sleep', 'morning'], negative=['night'], topn = 5)\n",
    "print(\"Similar words:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d6a167",
   "metadata": {},
   "source": [
    "<b>Q. What do you conclude from comparing vectors generated by yourself and the pretrained model? </b><br>\n",
    "A. Pretrained model has better vocab than our own model. Hence it captures better similarities of words in most of the cases. <br>\n",
    "As the vocab size increases, the similarities vector improvises.<br><br>\n",
    "<b>Q. Which of the Word2Vec models seems to encode semantic similarities between words better? </b><br>\n",
    "A. Pretrained model seems to encode semantic similarities between words better.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b829cc",
   "metadata": {},
   "source": [
    "## 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83110486",
   "metadata": {},
   "source": [
    "Created average vectors for each review. For this, first added all the vectors in each review in a list. Then, calculated the mean of those vectors using np.mean(). If there is no vector, we append 300 zeroes as a replacemet since the original vector will have 300 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd8cbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_vector = []\n",
    "\n",
    "for review in df['review_body1']:\n",
    "    vectors = []\n",
    "    for word in review:\n",
    "        if word in wv.key_to_index:\n",
    "            vectors.append(wv.get_vector(word))\n",
    "    \n",
    "    if len(vectors) > 0:\n",
    "        avg_vector.append(np.mean(vectors, axis = 0))\n",
    "    else:\n",
    "        avg_vector.append(np.zeros(300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e40e3c",
   "metadata": {},
   "source": [
    "Split the code into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b92a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(avg_vector, df['class'], stratify=df['class'], \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e879d8b",
   "metadata": {},
   "source": [
    "<b> Perceptron model </b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb23bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron model output:\n",
      "class1:  0.6569439840901558 ,  0.4955 ,  0.5649137808180134\n",
      "class2:  0.4934014474244359 ,  0.5795 ,  0.532996091055415\n",
      "class3:  0.6413068844807468 ,  0.687 ,  0.6633675316837658\n",
      "average: 0.5972174386651129 ,  0.5873333333333334 ,  0.5870924678523981\n"
     ]
    }
   ],
   "source": [
    "model_p = Perceptron(random_state=5)\n",
    "model_p.fit(X_train, Y_train)\n",
    "\n",
    "#Testing the model\n",
    "Y_pred = model_p.predict(X_test)\n",
    "\n",
    "precision_score_p = precision_score(Y_test, Y_pred, average=None)\n",
    "recall_score_p = recall_score(Y_test, Y_pred, average=None)\n",
    "f1_score_p = f1_score(Y_test, Y_pred, average=None)\n",
    "\n",
    "print(\"Perceptron model output:\")\n",
    "print(\"class1: \", precision_score_p[0], \", \", recall_score_p[0], \", \", f1_score_p[0])\n",
    "print(\"class2: \", precision_score_p[1], \", \", recall_score_p[1], \", \", f1_score_p[1])\n",
    "print(\"class3: \", precision_score_p[2], \", \", recall_score_p[2], \", \", f1_score_p[2])\n",
    "print(\"average:\", precision_score(Y_test, Y_pred, average='weighted'),\", \", recall_score(Y_test, Y_pred, average='weighted'),\n",
    "     \", \", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "713d03ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron model output with TF-IDF (Taken from HW1) : \n",
      " class1:  0.6009032564772997 ,  0.632 ,  0.6160594614353601 \n",
      " class2:  0.5062782521346058 ,  0.504 ,  0.5051365572538211 \n",
      " class3:  0.6688533193387562 ,  0.63725 ,  0.652669312508001 \n",
      " average: 0.5920116093168872 ,  0.5910833333333333 ,  0.5912884437323942\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron model output with TF-IDF (Taken from HW1) : \\n\",\n",
    "\"class1:  0.6009032564772997 ,  0.632 ,  0.6160594614353601 \\n\",\n",
    "\"class2:  0.5062782521346058 ,  0.504 ,  0.5051365572538211 \\n\",\n",
    "\"class3:  0.6688533193387562 ,  0.63725 ,  0.652669312508001 \\n\",\n",
    "\"average: 0.5920116093168872 ,  0.5910833333333333 ,  0.5912884437323942\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14b384",
   "metadata": {},
   "source": [
    "<b> SVM Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50d7df8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model output:\n",
      "class1:  0.6340898564150069 ,  0.6845 ,  0.6583313296465496\n",
      "class2:  0.5646145313366612 ,  0.509 ,  0.5353668156718381\n",
      "class3:  0.6874386653581943 ,  0.7005 ,  0.6939078751857355\n",
      "average: 0.6287143510366208 ,  0.6313333333333333 ,  0.6292020068347077\n"
     ]
    }
   ],
   "source": [
    "model_s = LinearSVC(random_state=5) \n",
    "model_s.fit(X_train, Y_train)\n",
    "\n",
    "#Testing the model\n",
    "Y_pred = model_s.predict(X_test)\n",
    "\n",
    "precision_score_s = precision_score(Y_test, Y_pred, average=None)\n",
    "recall_score_s = recall_score(Y_test, Y_pred, average=None)\n",
    "f1_score_s = f1_score(Y_test, Y_pred, average=None)\n",
    "\n",
    "print(\"SVM model output:\")\n",
    "print(\"class1: \", precision_score_s[0], \", \", recall_score_s[0], \", \", f1_score_s[0])\n",
    "print(\"class2: \", precision_score_s[1], \", \", recall_score_s[1], \", \", f1_score_s[1])\n",
    "print(\"class3: \", precision_score_s[2], \", \", recall_score_s[2], \", \", f1_score_s[2])\n",
    "print(\"average:\", precision_score(Y_test, Y_pred, average='weighted'),\", \", recall_score(Y_test, Y_pred, average='weighted'),\n",
    "     \", \", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ef1fa180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model output with TF-IDF (Taken from HW1): \n",
      " class1:  0.6601401982112642 ,  0.68275 ,  0.67125476219737 \n",
      " class2:  0.5764705882352941 ,  0.539 ,  0.5571059431524549 \n",
      " class3:  0.7215619694397284 ,  0.74375 ,  0.7324879970454267 \n",
      " average: 0.6527242519620956 ,  0.6551666666666667 ,  0.6536162341317505\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM model output with TF-IDF (Taken from HW1): \\n\",\n",
    "\"class1:  0.6601401982112642 ,  0.68275 ,  0.67125476219737 \\n\",\n",
    "\"class2:  0.5764705882352941 ,  0.539 ,  0.5571059431524549 \\n\",\n",
    "\"class3:  0.7215619694397284 ,  0.74375 ,  0.7324879970454267 \\n\", \n",
    "\"average: 0.6527242519620956 ,  0.6551666666666667 ,  0.6536162341317505\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f13e94",
   "metadata": {},
   "source": [
    "<b>Q. What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?</b><br>\n",
    "A. In case of Perceptron, both the models with TF-IDF and Word2Vec features give almost similar accuracies. <br>\n",
    "However, TF-IDF gives better accuracies than Word2Vec features in both of the models. This is because TF-IDF captures the context of local data whereas Word2Vec considers relationship with entire dataset. This is the reason TF-IDF is working better with smaller dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc61cc",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23523f38",
   "metadata": {},
   "source": [
    "### Q. 4a) FNN with average vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef338d98",
   "metadata": {},
   "source": [
    "1. Created Sequential model using Keras. Then added 2 hidden layers with parameters described in question. <br>\n",
    "2. Created final output model with 3 units since its a classification with 3 classes. <br>\n",
    "3. Compiled the model using 'categorical_crossentropy' loss. <br>\n",
    "4. Converted all training and testing data of average vectors from previous steps into numpy array to use it in further steps. <br>\n",
    "5. Converted output data into categorical data with mention of 3 classes. <br>\n",
    "6. Fit the model with 15 epochs and batch_size as 32. <br>\n",
    "7. Finally, evaluated the test accuracy of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1cbdd773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.8609 - accuracy: 0.5994 - val_loss: 0.8202 - val_accuracy: 0.6302\n",
      "Epoch 2/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8046 - accuracy: 0.6344 - val_loss: 0.8020 - val_accuracy: 0.6388\n",
      "Epoch 3/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7849 - accuracy: 0.6444 - val_loss: 0.8005 - val_accuracy: 0.6321\n",
      "Epoch 4/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7714 - accuracy: 0.6526 - val_loss: 0.7881 - val_accuracy: 0.6412\n",
      "Epoch 5/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7592 - accuracy: 0.6586 - val_loss: 0.7820 - val_accuracy: 0.6478\n",
      "Epoch 6/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7469 - accuracy: 0.6636 - val_loss: 0.7806 - val_accuracy: 0.6474\n",
      "Epoch 7/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7362 - accuracy: 0.6710 - val_loss: 0.7739 - val_accuracy: 0.6580\n",
      "Epoch 8/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7258 - accuracy: 0.6754 - val_loss: 0.7816 - val_accuracy: 0.6497\n",
      "Epoch 9/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7154 - accuracy: 0.6803 - val_loss: 0.7962 - val_accuracy: 0.6415\n",
      "Epoch 10/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7061 - accuracy: 0.6855 - val_loss: 0.7756 - val_accuracy: 0.6522\n",
      "Epoch 11/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6964 - accuracy: 0.6914 - val_loss: 0.7810 - val_accuracy: 0.6500\n",
      "Epoch 12/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6847 - accuracy: 0.6988 - val_loss: 0.7930 - val_accuracy: 0.6519\n",
      "Epoch 13/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6776 - accuracy: 0.7006 - val_loss: 0.7875 - val_accuracy: 0.6528\n",
      "Epoch 14/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6676 - accuracy: 0.7082 - val_loss: 0.8660 - val_accuracy: 0.6302\n",
      "Epoch 15/15\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6595 - accuracy: 0.7111 - val_loss: 0.8040 - val_accuracy: 0.6525\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.8040 - accuracy: 0.6525\n",
      "Test accuracy for FNN with average vectors:  0.6524999737739563\n"
     ]
    }
   ],
   "source": [
    "model_fnn = keras.models.Sequential()\n",
    "\n",
    "model_fnn.add(keras.layers.Dense(units=100, activation='relu', input_dim = 300))\n",
    "model_fnn.add(keras.layers.Dense(units=10, activation='relu'))\n",
    "model_fnn.add(keras.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "model_fnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train_array_fnn = np.array(X_train)\n",
    "X_test_array_fnn = np.array(X_test)\n",
    "Y_train_array_fnn = np.array(Y_train)\n",
    "Y_test_array_fnn = np.array(Y_test)\n",
    "\n",
    "Y_train_onehot_fnn = keras.utils.to_categorical(Y_train_array_fnn - 1, num_classes = 3)\n",
    "Y_test_onehot_fnn = keras.utils.to_categorical(Y_test_array_fnn - 1, num_classes = 3)\n",
    "\n",
    "model_fnn.fit(X_train_array_fnn, Y_train_onehot_fnn, epochs = 15, verbose = 1, validation_split = 0, \n",
    "         validation_data = (X_test_array_fnn, Y_test_onehot_fnn))\n",
    "\n",
    "test_loss, test_acc = model_fnn.evaluate(X_test_array_fnn, Y_test_onehot_fnn)\n",
    "\n",
    "print(\"Test accuracy for FNN with average vectors: \", test_acc)\n",
    "#predictions = model.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468447ab",
   "metadata": {},
   "source": [
    "### Q. 4b) FNN with concatenated vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06cd0a",
   "metadata": {},
   "source": [
    "Created concatenated vectors for each review. For this, first concatenated all the vectors in each review in a list. Then, if the length of vectors is less than 10, appended vectors with 300 zeroes to make atleast 10 vectors. Finally selected only first 10 vectors from the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d9968fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_vector = []\n",
    "\n",
    "for review in df['review_body1']:\n",
    "    vectors = []\n",
    "    i = 0\n",
    "    for word in review:\n",
    "        if word in wv.key_to_index:\n",
    "            vectors.append(wv.get_vector(word))\n",
    "        \n",
    "    if len(vectors) < 10:\n",
    "        count = len(vectors)      \n",
    "        while count <= 10:\n",
    "            vectors.append(np.zeros(300))\n",
    "            count += 1\n",
    "           \n",
    "    concat_vector.append(vectors[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c3f3b",
   "metadata": {},
   "source": [
    "1. Split the concatenated vectors into training and testing data. <br>\n",
    "2. Created Sequential model using Keras. Then added 2 hidden layers with parameters described in question. <br>\n",
    "3. Created final output model with 3 units since its a classification with 3 classes. <br>\n",
    "4. Compiled the model using 'categorical_crossentropy' loss. <br>\n",
    "5. Converted all training and testing data into numpy array to use it in further steps. <br>\n",
    "6. Reshaped the input arrays as per the need of model input. <br>\n",
    "7. Converted output data into categorical data with mention of 3 classes. <br>\n",
    "8. Fit the model with 10 epochs and batch_size as 32. <br>\n",
    "9. Finally, evaluated the test accuracy of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac06ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.9239 - accuracy: 0.5480 - val_loss: 0.8888 - val_accuracy: 0.5742\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.8193 - accuracy: 0.6191 - val_loss: 0.8865 - val_accuracy: 0.5814\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7185 - accuracy: 0.6801 - val_loss: 0.9395 - val_accuracy: 0.5715\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.5814 - accuracy: 0.7575 - val_loss: 1.0277 - val_accuracy: 0.5638\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.4342 - accuracy: 0.8273 - val_loss: 1.2254 - val_accuracy: 0.5584\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.3076 - accuracy: 0.8820 - val_loss: 1.4935 - val_accuracy: 0.5592\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2255 - accuracy: 0.9154 - val_loss: 1.7402 - val_accuracy: 0.5499\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1712 - accuracy: 0.9375 - val_loss: 1.9433 - val_accuracy: 0.5490\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1413 - accuracy: 0.9486 - val_loss: 2.2882 - val_accuracy: 0.5488\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1206 - accuracy: 0.9569 - val_loss: 2.4429 - val_accuracy: 0.5435\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 2.4429 - accuracy: 0.5435\n",
      "Test accuracy for FNN with concatenated vectors:  0.5435000061988831\n"
     ]
    }
   ],
   "source": [
    "X_train_fnn, X_test_fnn, Y_train_fnn, Y_test_fnn = train_test_split(concat_vector, df['class'], stratify=df['class'], \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_fnn = keras.models.Sequential()\n",
    "\n",
    "model_fnn.add(keras.layers.Dense(units=100, activation='relu', input_dim = 3000))\n",
    "model_fnn.add(keras.layers.Dense(units=10, activation='relu'))\n",
    "model_fnn.add(keras.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "model_fnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train_array_fnn = np.array(X_train_fnn)\n",
    "X_test_array_fnn = np.array(X_test_fnn)\n",
    "Y_train_array_fnn = np.array(Y_train_fnn)\n",
    "Y_test_array_fnn = np.array(Y_test_fnn)\n",
    "\n",
    "X_train_array_fnn = X_train_array_fnn.reshape(48000, 3000)\n",
    "X_test_array_fnn = X_test_array_fnn.reshape(12000, 3000)\n",
    "\n",
    "Y_train_onehot_fnn = keras.utils.to_categorical(Y_train_array_fnn - 1, num_classes = 3)\n",
    "Y_test_onehot_fnn = keras.utils.to_categorical(Y_test_array_fnn - 1, num_classes = 3)\n",
    "\n",
    "\n",
    "model_fnn.fit(X_train_array_fnn, Y_train_onehot_fnn, epochs = 10, verbose = 1, validation_split = 0, \n",
    "         validation_data = (X_test_array_fnn, Y_test_onehot_fnn))\n",
    "\n",
    "test_loss, test_acc = model_fnn.evaluate(X_test_array_fnn, Y_test_onehot_fnn)\n",
    "\n",
    "print(\"Test accuracy for FNN with concatenated vectors: \", test_acc)\n",
    "#predictions = model.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c8245",
   "metadata": {},
   "source": [
    "<b> Q. What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section? </b><br>\n",
    "A. FNN with average vectors give better accuracies than simple models. But the FNN with concatenated vectors does not give as good accuracy as simple models.\n",
    "This is because FNNs build more complex relationships than simple models. But in concatenated vectors, we are considering only first 10 words of reviews and dropping the rest. Hence, it affects the accuracy of model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b558009",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a0597",
   "metadata": {},
   "source": [
    "Created concatenated vectors for each review. For this, first concatenated indexes of all the vectors in each review in a list. Then, if the length of vectors is less than 10, appended vectors with zero to make atleast 20 vectors. Finally selected only first 20 vectors from the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2948af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_vector = []\n",
    "\n",
    "for review in df['review_body1']:\n",
    "    vectors = []\n",
    "    i = 0\n",
    "    for word in review:\n",
    "        if word in wv.key_to_index:\n",
    "            vectors.append(wv.key_to_index[word])\n",
    "    \n",
    "    \n",
    "    if len(vectors) < 20:\n",
    "        count = len(vectors)      \n",
    "        while count <= 20:\n",
    "            vectors.append(0)\n",
    "            count += 1\n",
    "           \n",
    "    rnn_vector.append(vectors[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ef16c",
   "metadata": {},
   "source": [
    "Split the concatenated vectors into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e12f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn, X_test_rnn, Y_train_rnn, Y_test_rnn = train_test_split(rnn_vector, df['class'], stratify=df['class'], \n",
    "                                                    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5c409",
   "metadata": {},
   "source": [
    "### Q. 5a) Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794acbbc",
   "metadata": {},
   "source": [
    "1. Created Sequential model using Keras. \n",
    "2. Added embedding layer into RNN to give vector embedding input. Then added RNNlayer. <br>\n",
    "3. Created final output model with 3 units since its a classification with 3 classes. <br>\n",
    "4. Compiled the model using 'categorical_crossentropy' loss. <br>\n",
    "5. Converted all training and testing data into numpy array to use it in further steps. <br>\n",
    "6. Converted output data into categorical data with mention of 3 classes. <br>\n",
    "7. Fit the model with 10 epochs and batch_size as 32. <br>\n",
    "8. Finally, evaluated the test accuracy of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nGnDtOj0Ng5l",
   "metadata": {
    "id": "nGnDtOj0Ng5l"
   },
   "outputs": [],
   "source": [
    "model_rnn = keras.models.Sequential()\n",
    "model_rnn.add(keras.layers.Embedding(input_dim = len(wv.vocab), output_dim = 300, input_length = 20, weights = [wv.vectors]))\n",
    "model_rnn.add(keras.layers.SimpleRNN(units=20, activation='relu'))\n",
    "model_rnn.add(keras.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array_rnn = np.array(X_train_rnn)\n",
    "X_test_array_rnn = np.array(X_test_rnn)\n",
    "Y_train_array_rnn = np.array(Y_train_rnn)\n",
    "Y_test_array_rnn = np.array(Y_test_rnn)\n",
    "\n",
    "Y_train_onehot_rnn = keras.utils.to_categorical(Y_train_array_rnn - 1, num_classes = 3)\n",
    "Y_test_onehot_rnn = keras.utils.to_categorical(Y_test_array_rnn - 1, num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XQYvYUv-OFQM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQYvYUv-OFQM",
    "outputId": "0b697a63-0cf0-45b2-fa51-a236313212e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 76s 51ms/step - loss: 1.0917 - accuracy: 0.3701 - val_loss: 1.0946 - val_accuracy: 0.3597\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 1.0866 - accuracy: 0.3768 - val_loss: 1.0922 - val_accuracy: 0.3578\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 1.0568 - accuracy: 0.4174 - val_loss: 1.0438 - val_accuracy: 0.4223\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 1.0334 - accuracy: 0.4384 - val_loss: 1.0252 - val_accuracy: 0.4497\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 73s 49ms/step - loss: 1.0202 - accuracy: 0.4479 - val_loss: 1.0195 - val_accuracy: 0.4513\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 73s 49ms/step - loss: 1.0128 - accuracy: 0.4571 - val_loss: 1.0147 - val_accuracy: 0.4532\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 73s 49ms/step - loss: 1.0075 - accuracy: 0.4602 - val_loss: 1.0097 - val_accuracy: 0.4588\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 1.0013 - accuracy: 0.4646 - val_loss: 1.0206 - val_accuracy: 0.4496\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 1.0006 - accuracy: 0.4635 - val_loss: 1.0062 - val_accuracy: 0.4653\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 0.9988 - accuracy: 0.4672 - val_loss: 1.0092 - val_accuracy: 0.4659\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 1.0092 - accuracy: 0.4659\n",
      "Test accuracy for Simple RNN:  0.4659166634082794\n"
     ]
    }
   ],
   "source": [
    "model_rnn.fit(X_train_array_rnn, Y_train_onehot_rnn, epochs = 10, validation_data = (X_test_array_rnn, Y_test_onehot_rnn))\n",
    "\n",
    "test_loss, test_acc = model_rnn.evaluate(X_test_array_rnn, Y_test_onehot_rnn)\n",
    "\n",
    "print(\"Test accuracy for Simple RNN: \", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247daee3",
   "metadata": {},
   "source": [
    "<b> Q. What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models? </b><br>\n",
    "A. FNN is giving better accuracy than RNN due to complexity of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8208e6",
   "metadata": {},
   "source": [
    "### Q. 5b) GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead96126",
   "metadata": {},
   "source": [
    "1. Created Sequential model using Keras. \n",
    "2. Added embedding layer into model to give vector embedding input. Then added GRU layer. <br>\n",
    "3. Created final output model with 3 units since its a classification with 3 classes. <br>\n",
    "4. Compiled the model using 'categorical_crossentropy' loss. <br>\n",
    "5. Fit the model with 10 epochs and batch_size as 32. <br>\n",
    "6. Finally, evaluated the test accuracy of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model_gru = keras.models.Sequential()\n",
    "model_gru.add(keras.layers.Embedding(input_dim = len(wv.key_to_index), output_dim = 300, input_length = 20, weights = [wv.vectors]))\n",
    "model_gru.add(keras.layers.GRU(units=20, activation='relu'))\n",
    "model_gru.add(keras.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "POtAVNMVeLG4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POtAVNMVeLG4",
    "outputId": "ab520001-217e-4519-9b83-2fc5089cabb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 86s 53ms/step - loss: 1.0844 - accuracy: 0.3854 - val_loss: 1.0572 - val_accuracy: 0.4243\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 76s 50ms/step - loss: 1.0415 - accuracy: 0.4393 - val_loss: 1.0271 - val_accuracy: 0.4533\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 76s 50ms/step - loss: 1.0169 - accuracy: 0.4657 - val_loss: 1.0244 - val_accuracy: 0.4631\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 76s 50ms/step - loss: 0.9987 - accuracy: 0.4818 - val_loss: 1.0001 - val_accuracy: 0.4843\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 76s 51ms/step - loss: 0.9849 - accuracy: 0.4925 - val_loss: 0.9927 - val_accuracy: 0.4863\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 76s 50ms/step - loss: 0.9744 - accuracy: 0.5018 - val_loss: 0.9885 - val_accuracy: 0.4945\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 76s 51ms/step - loss: 0.9663 - accuracy: 0.5098 - val_loss: 0.9786 - val_accuracy: 0.5037\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 76s 51ms/step - loss: 0.9593 - accuracy: 0.5120 - val_loss: 0.9808 - val_accuracy: 0.5025\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 76s 51ms/step - loss: 0.9541 - accuracy: 0.5164 - val_loss: 0.9757 - val_accuracy: 0.5038\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 76s 51ms/step - loss: 0.9489 - accuracy: 0.5215 - val_loss: 0.9769 - val_accuracy: 0.5033\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.9769 - accuracy: 0.5033\n",
      "Test accuracy for GRU:  0.503333330154419\n"
     ]
    }
   ],
   "source": [
    "model_gru.fit(X_train_array_rnn, Y_train_onehot_rnn, epochs = 10, validation_data = (X_test_array_rnn, Y_test_onehot_rnn))\n",
    "\n",
    "test_loss, test_acc = model_gru.evaluate(X_test_array_rnn, Y_test_onehot_rnn)\n",
    "\n",
    "print(\"Test accuracy for GRU: \", test_acc)\n",
    "\n",
    "# #predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06db437",
   "metadata": {},
   "source": [
    "### Q. 5c) LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c69f7",
   "metadata": {},
   "source": [
    "1. Created Sequential model using Keras. \n",
    "2. Added embedding layer into model to give vector embedding input. Then added LSTM layer. <br>\n",
    "3. Created final output model with 3 units since its a classification with 3 classes. <br>\n",
    "4. Compiled the model using 'categorical_crossentropy' loss. <br>\n",
    "5. Fit the model with 10 epochs and batch_size as 32. <br>\n",
    "6. Finally, evaluated the test accuracy of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t3ner5t9eaeA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3ner5t9eaeA",
    "outputId": "70bdb4ab-51b9-43a3-858d-9f573025019b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model_lstm = keras.models.Sequential()\n",
    "model_lstm.add(keras.layers.Embedding(input_dim = len(wv.key_to_index), output_dim = 300, input_length = 20, weights = [wv.vectors]))\n",
    "model_lstm.add(keras.layers.LSTM(units=20, activation='relu'))\n",
    "model_lstm.add(keras.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecK8WJlBel15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecK8WJlBel15",
    "outputId": "1a58ea1b-1690-44d5-910a-e75620e40b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 85s 52ms/step - loss: 1.0710 - accuracy: 0.4029 - val_loss: 1.0462 - val_accuracy: 0.4372\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 1.0401 - accuracy: 0.4435 - val_loss: 1.0276 - val_accuracy: 0.4579\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 74s 50ms/step - loss: 1.0208 - accuracy: 0.4620 - val_loss: 1.0128 - val_accuracy: 0.4617\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 75s 50ms/step - loss: 0.9987 - accuracy: 0.4816 - val_loss: 0.9988 - val_accuracy: 0.4823\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 0.9839 - accuracy: 0.4952 - val_loss: 0.9926 - val_accuracy: 0.4852\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 0.9732 - accuracy: 0.5029 - val_loss: 0.9912 - val_accuracy: 0.4927\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 0.9656 - accuracy: 0.5088 - val_loss: 0.9843 - val_accuracy: 0.4941\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 75s 50ms/step - loss: 0.9598 - accuracy: 0.5103 - val_loss: 0.9839 - val_accuracy: 0.4963\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 0.9528 - accuracy: 0.5181 - val_loss: 0.9786 - val_accuracy: 0.4967\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 74s 49ms/step - loss: 0.9479 - accuracy: 0.5200 - val_loss: 0.9779 - val_accuracy: 0.5002\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.9779 - accuracy: 0.5002\n",
      "0.9778949618339539   0.500249981880188\n"
     ]
    }
   ],
   "source": [
    "model_lstm.fit(X_train_array_rnn, Y_train_onehot_rnn, epochs = 10, validation_data = (X_test_array_rnn, Y_test_onehot_rnn))\n",
    "\n",
    "test_loss, test_acc = model_lstm.evaluate(X_test_array_rnn, Y_test_onehot_rnn)\n",
    "\n",
    "print(test_loss, \" \", test_acc)\n",
    "\n",
    "# #predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20610890",
   "metadata": {},
   "source": [
    "<b> Q. What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN? </b><br>\n",
    "GRU and LSTM gives better accuracies than simple RNN. This is because GRU and LSTM handles long term accuracies better than RNNs. Also, GRUs are less prone to overfitting while LST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69b4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
